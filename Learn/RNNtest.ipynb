{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bit40dc0dc0f4f8425f82bc2b10d91bf305",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[ 0.0569,  0.3682,  0.2133,  0.0476,  0.2376,  0.1589, -0.0052,\n            0.1590,  0.1072,  0.0216],\n          [ 0.1053,  0.0610,  0.2369,  0.0147,  0.2249,  0.1357,  0.0152,\n            0.0290,  0.0837,  0.1499],\n          [ 0.2017,  0.1203,  0.2076,  0.0205,  0.2493,  0.1089,  0.0089,\n           -0.0586,  0.1051, -0.0259],\n          [ 0.0917,  0.2015,  0.1172, -0.0227,  0.2657,  0.0710,  0.0733,\n            0.1241,  0.0521, -0.0917]],\n \n         [[ 0.0132,  0.2133,  0.1737,  0.0172,  0.2109,  0.0458, -0.0728,\n            0.0787, -0.0498, -0.1009],\n          [ 0.0277,  0.0638,  0.2087, -0.0159,  0.1455,  0.0272, -0.0667,\n            0.0025, -0.0551, -0.0341],\n          [ 0.1021,  0.0711,  0.1841, -0.0400,  0.1866,  0.0257, -0.0844,\n           -0.0382, -0.0356, -0.1085],\n          [ 0.0270,  0.1154,  0.1291, -0.0442,  0.1953, -0.0032, -0.0270,\n            0.0519, -0.0702, -0.1579]]], grad_fn=<StackBackward>),\n (tensor([[[-0.0189,  0.0300,  0.0944, -0.0563, -0.0623,  0.1557,  0.0512,\n             0.3408, -0.1951, -0.0411],\n           [ 0.0143,  0.0955,  0.0147,  0.1333, -0.1432,  0.2630,  0.1974,\n             0.3607, -0.0634, -0.1251],\n           [ 0.1623,  0.0152,  0.0679, -0.0661, -0.1189,  0.2727,  0.1656,\n             0.3275, -0.1616, -0.1127],\n           [ 0.0532, -0.0540,  0.1145,  0.0555, -0.1354,  0.1715,  0.0946,\n             0.3967, -0.1402, -0.0924]],\n  \n          [[ 0.1036,  0.3005, -0.0836,  0.1032,  0.0453,  0.0084,  0.1232,\n             0.0610,  0.0394,  0.1447],\n           [ 0.0171,  0.1517, -0.0548,  0.1490,  0.0162,  0.0080,  0.1407,\n             0.1406,  0.0149,  0.1987],\n           [-0.0426,  0.2970, -0.0398,  0.0754, -0.0333, -0.0209,  0.1797,\n             0.1261,  0.0510,  0.1303],\n           [ 0.0425,  0.2820, -0.0454,  0.1140,  0.0724, -0.0349,  0.1028,\n             0.1354,  0.0414,  0.1143]],\n  \n          [[ 0.0132,  0.2133,  0.1737,  0.0172,  0.2109,  0.0458, -0.0728,\n             0.0787, -0.0498, -0.1009],\n           [ 0.0277,  0.0638,  0.2087, -0.0159,  0.1455,  0.0272, -0.0667,\n             0.0025, -0.0551, -0.0341],\n           [ 0.1021,  0.0711,  0.1841, -0.0400,  0.1866,  0.0257, -0.0844,\n            -0.0382, -0.0356, -0.1085],\n           [ 0.0270,  0.1154,  0.1291, -0.0442,  0.1953, -0.0032, -0.0270,\n             0.0519, -0.0702, -0.1579]]], grad_fn=<StackBackward>),\n  tensor([[[-0.0369,  0.0664,  0.2474, -0.1372, -0.1263,  0.3221,  0.1203,\n             0.6207, -0.3400, -0.0887],\n           [ 0.0276,  0.2481,  0.0404,  0.2692, -0.3336,  0.5262,  0.5035,\n             0.6416, -0.1150, -0.2376],\n           [ 0.2867,  0.0367,  0.1648, -0.1507, -0.2421,  0.5155,  0.3944,\n             0.5422, -0.2670, -0.2323],\n           [ 0.0892, -0.1176,  0.2989,  0.1295, -0.2575,  0.3875,  0.2372,\n             0.6628, -0.2376, -0.2240]],\n  \n          [[ 0.2387,  0.4961, -0.1425,  0.2082,  0.1036,  0.0238,  0.3186,\n             0.1365,  0.0967,  0.2483],\n           [ 0.0381,  0.2466, -0.0865,  0.3082,  0.0319,  0.0221,  0.3935,\n             0.2921,  0.0365,  0.3581],\n           [-0.1031,  0.4998, -0.0630,  0.1490, -0.0683, -0.0541,  0.5015,\n             0.2488,  0.1283,  0.2386],\n           [ 0.0997,  0.4736, -0.0745,  0.2331,  0.1581, -0.0929,  0.2768,\n             0.2958,  0.0963,  0.2012]],\n  \n          [[ 0.0302,  0.4885,  0.3526,  0.0326,  0.4319,  0.0797, -0.1351,\n             0.1572, -0.0856, -0.1818],\n           [ 0.0597,  0.1373,  0.4328, -0.0296,  0.3081,  0.0468, -0.1247,\n             0.0050, -0.0935, -0.0608],\n           [ 0.2425,  0.1528,  0.3831, -0.0737,  0.3946,  0.0469, -0.1648,\n            -0.0748, -0.0589, -0.1990],\n           [ 0.0609,  0.2467,  0.2557, -0.0824,  0.3958, -0.0056, -0.0507,\n             0.1004, -0.1158, -0.2888]]], grad_fn=<StackBackward>)))"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# 实现一个num_layers层的LSTM-RNN\n",
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, num_layers):\n",
    "        super(RNN,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_layers=num_layers\n",
    "        self.lstm = torch.nn.LSTM(input_size=input_size,hidden_size=hidden_size,num_layers=num_layers,batch_first=False)\n",
    "    \n",
    "    def forward(self,input):\n",
    "        # input应该为(batch_size,seq_len,input_szie)\n",
    "        self.hidden = self.initHidden(input.size(1))\n",
    "        out,self.hidden = self.lstm(input,self.hidden)\n",
    "        return out,self.hidden\n",
    "    \n",
    "    def initHidden(self,batch_size):\n",
    "        if self.lstm.bidirectional:\n",
    "            return (torch.rand(self.num_layers*2,batch_size,self.hidden_size),torch.rand(self.num_layers*2,batch_size,self.hidden_size))\n",
    "        else:\n",
    "            return (torch.rand(self.num_layers,batch_size,self.hidden_size),torch.rand(self.num_layers,batch_size,self.hidden_size))\n",
    "\n",
    "input_size = 12\n",
    "hidden_size = 10\n",
    "num_layers = 3\n",
    "batch_size = 2\n",
    "model = RNN(input_size,hidden_size,num_layers)\n",
    "# input (seq_len, batch, input_size) 包含特征的输入序列，如果设置了batch_first，则batch为第一维\n",
    "input = torch.rand(2,4,12)\n",
    "model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "torch.Size([2, 3, 12, 13])\n"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "m = nn.Softmax2d()\n",
    "# you softmax over the 2nd dimension\n",
    "input = torch.randn(2, 3, 12, 13)\n",
    "output = m(input)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([1., 2.])\ntorch.Size([1, 2])\n"
    }
   ],
   "source": [
    "import torch\n",
    " \n",
    "a = torch.Tensor([1,2])\n",
    "a1 = a.unsqueeze(0)\n",
    "print(a)\n",
    "print(a1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}