{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习课上内容， 阅读相应论文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答以下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  What is autoencoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ans:总的来说就是把一个信息压缩，然后再从压缩信息还原到原始信息的一个过程。\n",
    "\n",
    "把原始信息传入第一个神经网络，得到一个结果，这个结果叫隐空间，这一步叫做压缩，这个隐空间的维度(大小)比原始要小并且包含原始的信息；然后把隐空间传入另一个神经网络，得到一个结果，这个结果会尽量和原始信息保持一致，这一步叫做还原。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What are the differences between greedy search and beam search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "asn：greedy search:在RNN处理序列的过程中，greedy search对于序列的每一个元素都取概率最大的结果往下个节点传递，最后得到的结果是所有元素的最大结果的拼接\n",
    "\n",
    "beam search:在RNN处理序列的过程中，beam search对于序列的每一个元素都取概率前N大的结果往下个节点传递，由于每个节点都取前N大的结果，最后会形成一个树形结构，最后得到的结果需要从这个树形结构中找到权重最大的分支"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the intuition of attention mechanism?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ans:在RNN处理一个序列中，序列中的某些节点对于这个序列不重要，这时就需要一个权重值去区分这些节点的重要程度，这种思想叫做注意力机制。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the disadvantage of word embeding introduced in previous lectures ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ans：没有解决一词多义的问题，一个词只有一个固定的词向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What is the architecture of ELMo model. (A brief description is enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ans:一个双层双向的LSTM模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Compared to RNN,  what is the advantage of Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ans: \n",
    "\n",
    "RNN必须要先计算前一个节点，把前一个节点处理完的结果传入下一个节点\n",
    "\n",
    "Transformer可以不用拆分序列一次处理整个序列，提高的执行效率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Why we use layer normalizaiton instead of batch normalization in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ans:batch normalization是对一批数据的维度进行归一化，如果这批数据没逻辑上的关联性，这种归一化是没有意义的，不能提取出重要的信息，而layer normalizaiton是对一批数据中的一个样本沿着词向量的方向进行标准化，会提取这句话中每个词向量的重要信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Why we need position embedding in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ans:由于Transformer是可以不用拆分序列进行并行计算的，但就是因为这种并行计算，丢失了序列之间的前后位置信息，所以引入了position embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Briefly describe what is self-attention and what is multi-head attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ans:\n",
    "\n",
    "self-attention：对词向量进行三个权重的处理，得到的结果会把与这个词更相近的词标注出更高的权重，类似于字典用其他相近的词语去解释一个词。\n",
    "\n",
    "multi-head attention：做了多次的self-attention，并把这些结果拼接后，用全连接神经网络处理得到原始的词向量维度。multi-head会汇总了self-attention的所有信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. What is the basic unit of GPT model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ans:GPT的基础单元是decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Briefly descibe how to use GPT in other NLP tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ans:总的来说需要使用masker self-attention，即当前词只能与前面的词做self-attention，不能看到后面的词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. What is masked language model in BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ans:由于Bert的基础单元是encode，encode是可以看到所有的词，所以他把一个序列中间的位置进行挖空，使用mask去代替，然后做正常的attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. What are the inputs of BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ans:1.使用SEP把句子隔开 2.使用cls标注所有的句子 3.并把不同的句子使用segment标记出来 3.随机初始化位置编码（根据训练去学习）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Briely descibe how to use BERT in other NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "asn:\n",
    "\n",
    "1. 分类任务，把得到的cls结果连接linear层，最后做softmax得到分类结果\n",
    "\n",
    "2. 最句子中的每个词进行标注时，需要把对每个词的结果进行处理即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. What are the differences between these three models: GPT, BERT, GPT2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ans:\n",
    "\n",
    "GPT：单向的transfermer，以decoder为基础单元，而且必须把上一个词的输出作为下一个词的输入，因为使用了decoder，在计算某个词attention时需要把当前词之后词遮盖，别面后续的词语影响输出。\n",
    "\n",
    "Bert：双向的transfermer，以encoder为基础单元，需要给输入的句子的开头赋CLS标签，把每个句子用SEP分割，并使用mask随机以一定概率替换句子中的某些词。\n",
    "\n",
    "GPT2：和GPT结构一样，只是比GPT的结构更大，训练数据更多，质量更高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}